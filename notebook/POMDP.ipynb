{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b384c9",
   "metadata": {},
   "source": [
    "# Simple POMDP example with two states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fce95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e57fde",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_b_m(r):\n",
    "    b = r[0]\n",
    "    m = r[1] - r[0]\n",
    "    return b, m\n",
    "    \n",
    "def compute_intersect(r1, r2):\n",
    "    b1, m1 = get_b_m(r1)\n",
    "    b2, m2 = get_b_m(r2)\n",
    "    if np.isclose(m2 - m1, 0):\n",
    "        return None\n",
    "    return (b2 - b1)/(m1 - m2)\n",
    "   \n",
    "def compute_segment(r, rewards):\n",
    "    lb, ub = 0, 1\n",
    "    for r_prime in rewards:\n",
    "        if np.all(r >= r_prime):\n",
    "            continue\n",
    "        x = compute_intersect(r, r_prime)\n",
    "        if x is not None and 0 <= x <= 1:\n",
    "            _, m1 = get_b_m(r)\n",
    "            _, m2 = get_b_m(r_prime)\n",
    "            if m1 > m2:\n",
    "                lb = max(lb, x)\n",
    "            else:\n",
    "                ub = min(ub, x)\n",
    "    return lb, ub\n",
    "\n",
    "def compute_parsimonious(rewards):\n",
    "    parsimonious = []\n",
    "    for r in rewards:\n",
    "        lb, ub = compute_segment(r, rewards)\n",
    "        if lb > ub or np.isclose(lb, ub):\n",
    "            continue\n",
    "        parsimonious.append(r)\n",
    "    return parsimonious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c59643",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c873842",
   "metadata": {},
   "source": [
    "### States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0, s1 = 0, 1\n",
    "states = [s0, s1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b67243",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa49a51",
   "metadata": {},
   "source": [
    "There are two possible observations, o0 and o1. Z specifies the probability of observing o0 and o1 in a particular state. Note that the probability does not depend on the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "o0, o1 = \"a\", \"b\"\n",
    "obs = [o0, o1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = dict()\n",
    "Z[(s0, o0)] = 0.7\n",
    "Z[(s0, o1)] = 0.3\n",
    "Z[(s1, o0)] = 0.4\n",
    "Z[(s1, o1)] = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69f3c5",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745d0f5",
   "metadata": {},
   "source": [
    "There are two actions available in both states, with R capturing the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88154f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, a1 = \"a0\", \"a1\"\n",
    "actions = [a0, a1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = dict()\n",
    "R[(s0, a0)] = 1\n",
    "R[(s1, a0)] = 2\n",
    "R[(s0, a1)] = 3\n",
    "R[(s1, a1)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd4db6",
   "metadata": {},
   "source": [
    "### Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61101069",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = dict()\n",
    "\n",
    "# Action a0\n",
    "P[(s0, a0, s0)] = 0.9\n",
    "P[(s0, a0, s1)] = 0.1\n",
    "P[(s1, a0, s0)] = 0.2\n",
    "P[(s1, a0, s1)] = 0.8\n",
    "\n",
    "# Action a1\n",
    "P[(s0, a1, s0)] = 0.3\n",
    "P[(s0, a1, s1)] = 0.7\n",
    "P[(s1, a1, s0)] = 0.6\n",
    "P[(s1, a1, s1)] = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438ee33",
   "metadata": {},
   "source": [
    "## Belief updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_prob(s, a, o):\n",
    "    \"\"\"Computes the probability of observing o after playing action a in s\"\"\"\n",
    "    return sum([P[(s, a, s_prime)] * Z[(s_prime, o)] for s_prime in states])\n",
    "\n",
    "print(obs_prob(s0, a0, o0))\n",
    "print(obs_prob(s1, a0, o0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def belief_T(b, a, o):\n",
    "    n = sum(b[s]*obs_prob(s, a, o) for s in states)\n",
    "    M = np.array([\n",
    "        [Z[(s0, o)] * P[(s0, a, s0)], Z[(s0, o)] * P[(s1, a, s0)]],\n",
    "        [Z[(s1, o)] * P[(s0, a, s1)], Z[(s1, o)] * P[(s1, a, s1)]]\n",
    "        ]) * (1/n)\n",
    "    return np.matmul(M, b)\n",
    "    \n",
    "print(belief_T(np.array([0.5, 0.5]), a0, o0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3a1db",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [np.array([R[s0, a], R[s1, a]]) for a in actions]\n",
    "\n",
    "def plot_reward(r, label=None, color=None):\n",
    "    plt.plot([0, 1], r, label=label, color=color)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.xlabel(\"P(s=s1)\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    \n",
    "for c, a, r in zip(mcolors.TABLEAU_COLORS, actions, rewards):\n",
    "    plot_reward(r, label=a, color=c)\n",
    "    lb, ub = compute_segment(r, rewards)\n",
    "    plt.axhline(y=0, xmin=lb, xmax=ub, color=c, linewidth=6)\n",
    "plt.legend()\n",
    "plt.ylim(ymin=0)\n",
    "plt.savefig(\"instant-reward.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_reward_obs(r, a, o):\n",
    "    M = np.array([\n",
    "        [Z[(s0, o)] * P[(s0, a, s0)], Z[(s1, o)] * P[(s0, a, s1)]],\n",
    "        [Z[(s0, o)] * P[(s1, a, s0)], Z[(s1, o)] * P[(s1, a, s1)]]\n",
    "        ])\n",
    "    v = np.matmul(M, r)\n",
    "    return v\n",
    "\n",
    "def transform_reward(r, a, cumulative=False):\n",
    "    vectors = []\n",
    "    for o in obs:\n",
    "        vectors.append(transform_reward_obs(r, a, o))\n",
    "    if cumulative:\n",
    "        vectors.append(np.array([R[s0, a], R[s1, a]]))\n",
    "    return np.sum(vectors, axis=0)\n",
    "\n",
    "def get_reward_val(r, b):\n",
    "    return np.dot(r, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_rewards = [transform_reward_obs(r, a0, o0) for r in rewards]\n",
    "for c, a, r in zip(mcolors.TABLEAU_COLORS, actions, transformed_rewards):\n",
    "    plot_reward(r, label=f\"Action {a}\")\n",
    "    lb, ub = compute_segment(r, transformed_rewards)\n",
    "    plt.axhline(y=0, xmin=lb, xmax=ub, color=c, linewidth=6)\n",
    "plt.legend()\n",
    "plt.ylim(ymin=0)\n",
    "plt.title(\"Assuming we play a_0 and observe o_0 in the first step\")\n",
    "plt.savefig(\"transformed-value.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3eeac4",
   "metadata": {},
   "source": [
    "### 3. Fix action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9999a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_rewards = [transform_reward(r, a0) for r in rewards]\n",
    "for c, a, r in zip(mcolors.TABLEAU_COLORS, actions, transformed_rewards):\n",
    "    plot_reward(r, label=f\"Action {a}\", color=c)\n",
    "    lb, ub = compute_segment(r, transformed_rewards)\n",
    "    plt.axhline(y=0, xmin=lb, xmax=ub, color=c, linewidth=6)\n",
    "plt.legend()\n",
    "plt.ylim(ymin=0)\n",
    "plt.title(\"Assuming we play a_0 first step\")\n",
    "plt.savefig(\"transformed-value-action.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a1d0e",
   "metadata": {},
   "source": [
    "### 4. Consider all actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dca173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "transformed_rewards = dict()\n",
    "for a, r in zip(actions, rewards):\n",
    "    for init_a in actions:\n",
    "        transformed_rewards[(a, init_a)] = (transform_reward(r, init_a, cumulative=True))\n",
    "\n",
    "for c, (a, a_init) in zip(mcolors.TABLEAU_COLORS, itertools.product(actions, actions)):\n",
    "    r = transformed_rewards[(a, a_init)]\n",
    "    plot_reward(r, color=c, label=f\"Playing {init_a} then {a}\")\n",
    "    lb, ub = compute_segment(r, transformed_rewards.values())\n",
    "    plt.axhline(y=0, xmin=lb, xmax=ub, color=c, linewidth=10)\n",
    "    \n",
    "plt.ylim(ymin=0)\n",
    "plt.legend()\n",
    "plt.title(\"Cumulative reward\")\n",
    "plt.savefig(\"nothing-fixed.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8c948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-kernel",
   "language": "python",
   "name": "rl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
